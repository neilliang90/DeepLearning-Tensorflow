{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager Execution\n",
    "https://www.tensorflow.org/get_started/eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\software\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "class MNISTModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(MNISTModel, self).__init__()\n",
    "    self.dense1 = tf.keras.layers.Dense(units=10)\n",
    "    self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "  def call(self, input):\n",
    "    \"\"\"Run the model.\"\"\"\n",
    "    result = self.dense1(input)\n",
    "    result = self.dense2(result)\n",
    "    result = self.dense2(result)  # reuse variables from dense2 layer\n",
    "    return result\n",
    "\n",
    "model = MNISTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data\n",
    "import dataset  # download dataset.py file\n",
    "dataset_train = dataset.train('./datasets').shuffle(60000).repeat(4).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 2.611\n",
      "Loss at step 0000: 2.668\n",
      "Loss at step 0200: 2.145\n",
      "Loss at step 0400: 2.047\n",
      "Loss at step 0600: 1.950\n",
      "Loss at step 0800: 1.909\n",
      "Loss at step 1000: 1.511\n",
      "Loss at step 1200: 1.546\n",
      "Loss at step 1400: 1.536\n",
      "Loss at step 1600: 1.585\n",
      "Loss at step 1800: 1.469\n",
      "Loss at step 2000: 1.266\n",
      "Loss at step 2200: 1.170\n",
      "Loss at step 2400: 1.338\n",
      "Loss at step 2600: 1.114\n",
      "Loss at step 2800: 1.282\n",
      "Loss at step 3000: 1.143\n",
      "Loss at step 3200: 0.817\n",
      "Loss at step 3400: 0.808\n",
      "Loss at step 3600: 0.708\n",
      "Loss at step 3800: 1.055\n",
      "Loss at step 4000: 0.870\n",
      "Loss at step 4200: 0.810\n",
      "Loss at step 4400: 0.769\n",
      "Loss at step 4600: 0.863\n",
      "Loss at step 4800: 0.701\n",
      "Loss at step 5000: 0.787\n",
      "Loss at step 5200: 0.749\n",
      "Loss at step 5400: 0.668\n",
      "Loss at step 5600: 0.867\n",
      "Loss at step 5800: 0.723\n",
      "Loss at step 6000: 0.536\n",
      "Loss at step 6200: 0.528\n",
      "Loss at step 6400: 0.535\n",
      "Loss at step 6600: 0.684\n",
      "Loss at step 6800: 0.756\n",
      "Loss at step 7000: 0.629\n",
      "Loss at step 7200: 0.488\n",
      "Loss at step 7400: 0.926\n",
      "Final loss: 0.670\n"
     ]
    }
   ],
   "source": [
    "def loss(model, x, y):\n",
    "  prediction = model(x)\n",
    "  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=prediction)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, model.variables)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "x, y = iter(dataset_train).next()\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, x, y)))\n",
    "\n",
    "# Training loop\n",
    "for (i, (x, y)) in enumerate(dataset_train):\n",
    "  # Calculate derivatives of the input function with respect to its parameters.\n",
    "  grads = grad(model, x, y)\n",
    "  # Apply the gradient to the model\n",
    "  optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "  if i % 200 == 0:\n",
    "    print(\"Loss at step {:04d}: {:.3f}\".format(i, loss(model, x, y)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with tf.GradeintTape function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 68.071\n",
      "Loss at step 000: 65.429\n",
      "Loss at step 020: 29.849\n",
      "Loss at step 040: 13.919\n",
      "Loss at step 060: 6.787\n",
      "Loss at step 080: 3.592\n",
      "Loss at step 100: 2.161\n",
      "Loss at step 120: 1.520\n",
      "Loss at step 140: 1.233\n",
      "Loss at step 160: 1.104\n",
      "Loss at step 180: 1.047\n",
      "Final loss: 1.022\n",
      "W = 2.9865317344665527, B = 2.1900746822357178\n"
     ]
    }
   ],
   "source": [
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "  return input * weight + bias\n",
    "\n",
    "# A loss function using mean-squared error\n",
    "def loss(weights, biases):\n",
    "  error = prediction(training_inputs, weights, biases) - training_outputs\n",
    "  return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# Return the derivative of loss with respect to weight and bias\n",
    "def grad(weights, biases):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(weights, biases)\n",
    "  return tape.gradient(loss_value, [weights, biases])\n",
    "\n",
    "train_steps = 200\n",
    "learning_rate = 0.01\n",
    "# Start with arbitrary values for W and B on the same batch of data\n",
    "W = tfe.Variable(5.)\n",
    "B = tfe.Variable(10.)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(W, B)))\n",
    "\n",
    "for i in range(train_steps):\n",
    "  dW, dB = grad(W, B)\n",
    "  W.assign_sub(dW * learning_rate)\n",
    "  B.assign_sub(dB * learning_rate)\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(W, B)))\n",
    "print(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.625\n",
      "Loss at step 000: 66.865\n",
      "Loss at step 020: 29.978\n",
      "Loss at step 040: 13.743\n",
      "Loss at step 060: 6.595\n",
      "Loss at step 080: 3.447\n",
      "Loss at step 100: 2.060\n",
      "Loss at step 120: 1.448\n",
      "Loss at step 140: 1.179\n",
      "Loss at step 160: 1.060\n",
      "Loss at step 180: 1.007\n",
      "Loss at step 200: 0.984\n",
      "Loss at step 220: 0.974\n",
      "Loss at step 240: 0.969\n",
      "Loss at step 260: 0.967\n",
      "Loss at step 280: 0.966\n",
      "Final loss: 0.966\n",
      "W = 2.971496105194092, B = 2.0467147827148438\n"
     ]
    }
   ],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.W = tfe.Variable(5., name='weight')\n",
    "    self.B = tfe.Variable(10., name='bias')\n",
    "  def predict(self, inputs):\n",
    "    return inputs * self.W + self.B\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 2000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# The loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "  error = model.predict(inputs) - targets\n",
    "  return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, [model.W, model.B])\n",
    "\n",
    "# Define:\n",
    "# 1. A model.\n",
    "# 2. Derivatives of a loss function with respect to model parameters.\n",
    "# 3. A strategy for updating the variables based on the derivatives.\n",
    "model = Model()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "# Training loop\n",
    "for i in range(300):\n",
    "  grads = grad(model, training_inputs, training_outputs)\n",
    "  optimizer.apply_gradients(zip(grads, [model.W, model.B]),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfe.num_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
